{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Advanced Spark"
      ],
      "metadata": {
        "id": "Z_VGk9alXB04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data lake is a centralized storage system that lets you:\n",
        "\n",
        "* Ingest and store raw data from any source (structured, semi-structured, unstructured)\n",
        "\n",
        "* Transform it using big data engines like Apache Spark\n",
        "\n",
        "* Make it queryable via tools like Hive, Presto, Spark SQL, or Delta Lake\n",
        "\n",
        "Hive: It lets you query large datasets using a language similar to SQL, called HiveQL."
      ],
      "metadata": {
        "id": "r3YN1yoqpS-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "z7t9VI96QxEB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Example\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load CSV with Schema"
      ],
      "metadata": {
        "id": "Y2qIkVV3Sv_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"sales.txt\", header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_CKTG10Rup8",
        "outputId": "34058867-ba0b-4790-acb2-4ed1fc649f42"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- sales: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            "\n",
            "+----------+------+-----+----------+\n",
            "|      date|region|sales|product_id|\n",
            "+----------+------+-----+----------+\n",
            "|2024-01-01| North|  100|       101|\n",
            "|2024-01-01| South|  200|       102|\n",
            "|2024-01-02| North|  150|       101|\n",
            "|2024-01-02| South|  300|       103|\n",
            "|2024-01-03|  East|  250|       104|\n",
            "|2024-01-03|  West|  180|       105|\n",
            "|2024-01-04| North|   90|       106|\n",
            "|2024-01-04|  East|  200|       104|\n",
            "|2024-01-05| South|  210|       102|\n",
            "|2024-01-05|  West|  300|       107|\n",
            "+----------+------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or use manual schema (better performance and safety)"
      ],
      "metadata": {
        "id": "JAd0_prcSzFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"date\", DateType(), True),\n",
        "    StructField(\"region\", StringType(), True),\n",
        "    StructField(\"sales\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df = spark.read.csv(\"sales.txt\", header=True, schema=schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JofMFwEnS1DM",
        "outputId": "71300df8-4cfe-4034-d9cb-89208335a2dd"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- sales: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            "\n",
            "+----------+------+-----+----------+\n",
            "|      date|region|sales|product_id|\n",
            "+----------+------+-----+----------+\n",
            "|2024-06-08| North| NULL|       345|\n",
            "|2024-06-26|  West| NULL|       372|\n",
            "|2024-06-20|  East| NULL|       304|\n",
            "|2024-06-12| North| NULL|       324|\n",
            "|2024-06-24| North| NULL|       312|\n",
            "|2024-06-06|  East| NULL|       340|\n",
            "|2024-06-02| South| NULL|       321|\n",
            "|2024-06-22|  West| NULL|       363|\n",
            "|2024-06-17| North| NULL|       345|\n",
            "|2024-06-16|  East| NULL|       306|\n",
            "|2024-06-07|  East| NULL|       328|\n",
            "|2024-06-01|  West| NULL|       396|\n",
            "|2024-06-08|  East| NULL|       310|\n",
            "|2024-06-15| North| NULL|       341|\n",
            "|2024-06-14|  East| NULL|       315|\n",
            "|2024-06-20|  West| NULL|       389|\n",
            "|2024-06-19|  West| NULL|       368|\n",
            "|2024-06-08|  East| NULL|       400|\n",
            "|2024-06-17| South| NULL|       400|\n",
            "|2024-06-08| North| NULL|       319|\n",
            "+----------+------+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `True` means it is allowed to contain null (missing) values."
      ],
      "metadata": {
        "id": "l7jphLOeTCis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Basic Aggregation: Sum of sales per region."
      ],
      "metadata": {
        "id": "4cLeVtZYTH_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"region\").sum(\"sales\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2-HZnY1TEta",
        "outputId": "6a7b4770-819c-49e0-b5f8-91d67ac066bf"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+\n",
            "|region|sum(sales)|\n",
            "+------+----------+\n",
            "| South|       710|\n",
            "|  East|       450|\n",
            "|  West|       480|\n",
            "| North|       340|\n",
            "+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import round, sum\n",
        "\n",
        "df.groupBy(\"region\") \\\n",
        "  .agg(round(sum(\"sales\"), 2).alias(\"total_sales\")) \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P4SgZO8czGL",
        "outputId": "935d881b-efc8-4e34-ec0b-0d5b61046499"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "|region|total_sales|\n",
            "+------+-----------+\n",
            "| South|        710|\n",
            "|  East|        450|\n",
            "|  West|        480|\n",
            "| North|        340|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate the SQL script."
      ],
      "metadata": {
        "id": "zMHdfmBdTWFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"sales_view\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT region, ROUND(SUM(sales), 2) AS sum_sales\n",
        "    FROM sales_view\n",
        "    GROUP BY region\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OLGnzxWTZh5",
        "outputId": "f3b23972-1300-4497-c7ae-057ac72e77b9"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|region|sum_sales|\n",
            "+------+---------+\n",
            "| South|      710|\n",
            "|  East|      450|\n",
            "|  West|      480|\n",
            "| North|      340|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note:\n",
        "`df.groupBy().sum()` automatically names the column sum(sales)\n",
        "> In SQL, you can alias it using AS (e.g., AS sum_sales)"
      ],
      "metadata": {
        "id": "XiW90gqYTe4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Window Function â€” Rolling Total"
      ],
      "metadata": {
        "id": "RzsTEmskTNxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window: Defines a window frame over which aggregations are calculated.\n",
        "\n"
      ],
      "metadata": {
        "id": "yGRI3YxiTokV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "windowSpec = Window.partitionBy(\"region\").orderBy(\"date\") \\\n",
        "                   .rowsBetween(Window.unboundedPreceding, 0)\n",
        "\n",
        "df.withColumn(\"cumulative_sales\", sum(\"sales\").over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL31TTUWTPNb",
        "outputId": "fba8e833-9307-47b1-d1c0-9ec7fefe694b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----------+----------------+\n",
            "|      date|region|sales|product_id|cumulative_sales|\n",
            "+----------+------+-----+----------+----------------+\n",
            "|2024-01-03|  East|  250|       104|             250|\n",
            "|2024-01-04|  East|  200|       104|             450|\n",
            "|2024-01-01| North|  100|       101|             100|\n",
            "|2024-01-02| North|  150|       101|             250|\n",
            "|2024-01-04| North|   90|       106|             340|\n",
            "|2024-01-01| South|  200|       102|             200|\n",
            "|2024-01-02| South|  300|       103|             500|\n",
            "|2024-01-05| South|  210|       102|             710|\n",
            "|2024-01-03|  West|  180|       105|             180|\n",
            "|2024-01-05|  West|  300|       107|             480|\n",
            "+----------+------+-----+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: 3-row Moving Average of sales (centered)"
      ],
      "metadata": {
        "id": "UMiRxe6UURsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "windowSpec = Window.partitionBy(\"region\").orderBy(\"date\") \\\n",
        "                   .rowsBetween(-1, 1)  # 1 row before, current row, 1 row after\n",
        "\n",
        "df.withColumn(\"moving_avg_sales\", avg(\"sales\").over(windowSpec)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfnj-dcWUSIm",
        "outputId": "176139a6-5e83-442c-e958-1cf937ce74b5"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----------+------------------+\n",
            "|      date|region|sales|product_id|  moving_avg_sales|\n",
            "+----------+------+-----+----------+------------------+\n",
            "|2024-01-03|  East|  250|       104|             225.0|\n",
            "|2024-01-04|  East|  200|       104|             225.0|\n",
            "|2024-01-01| North|  100|       101|             125.0|\n",
            "|2024-01-02| North|  150|       101|113.33333333333333|\n",
            "|2024-01-04| North|   90|       106|             120.0|\n",
            "|2024-01-01| South|  200|       102|             250.0|\n",
            "|2024-01-02| South|  300|       103|236.66666666666666|\n",
            "|2024-01-05| South|  210|       102|             255.0|\n",
            "|2024-01-03|  West|  180|       105|             240.0|\n",
            "|2024-01-05|  West|  300|       107|             240.0|\n",
            "+----------+------+-----+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> For each row, include all rows from the beginning of the partition up to the current row â†’ this is what makes it cumulative.\n",
        "\n",
        "> Alternative: 3-row trailing moving average\n",
        "> `Window.rowsBetween(-2, 0)`"
      ],
      "metadata": {
        "id": "menyJopkUN8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Joins sales with product descriptions.\n",
        ">\n",
        "> * how=\"left\" keeps all sales rows even if product name is missing.\n",
        "\n"
      ],
      "metadata": {
        "id": "nsJgyjTZU6Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_df = spark.read.csv(\"products.txt\", header=True, inferSchema=True)\n",
        "joined_df = df.join(products_df, on=\"product_id\", how=\"left\")\n",
        "joined_df.select(\"date\", \"region\", \"sales\", \"product_name\", \"price\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM1zYEzTUOto",
        "outputId": "c82aad37-bf8a-4f06-f294-92e07a8279f6"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+------------+------+\n",
            "|      date|region|sales|product_name| price|\n",
            "+----------+------+-----+------------+------+\n",
            "|2024-01-01| North|  100|       Phone|599.99|\n",
            "|2024-01-01| South|  200|      Tablet|399.99|\n",
            "|2024-01-02| North|  150|       Phone|599.99|\n",
            "|2024-01-02| South|  300|      Laptop|999.99|\n",
            "|2024-01-03|  East|  250|     Monitor|199.99|\n",
            "|2024-01-03|  West|  180|    Keyboard| 49.99|\n",
            "|2024-01-04| North|   90|       Mouse| 29.99|\n",
            "|2024-01-04|  East|  200|     Monitor|199.99|\n",
            "|2024-01-05| South|  210|      Tablet|399.99|\n",
            "|2024-01-05|  West|  300|     Printer|149.99|\n",
            "+----------+------+-----+------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: SQL Query on DataFrame"
      ],
      "metadata": {
        "id": "urH5Gdz7VA-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.createOrReplaceTempView(\"sales_view\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT region, SUM(sales) as total_sales\n",
        "    FROM sales_view\n",
        "    WHERE date >= '2024-01-02'\n",
        "    GROUP BY region\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOTgMY5kU9dO",
        "outputId": "315b2ae9-7090-41e4-de79-eea6078b067d"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "|region|total_sales|\n",
            "+------+-----------+\n",
            "| South|        510|\n",
            "|  East|        450|\n",
            "|  West|        480|\n",
            "| North|        240|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Let's join the data, and create a join query."
      ],
      "metadata": {
        "id": "gsmt6DRaVMFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"sales_view\")\n",
        "products_df.createOrReplaceTempView(\"products_view\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        s.date,\n",
        "        s.region,\n",
        "        s.sales,\n",
        "        s.product_id,\n",
        "        p.product_name\n",
        "    FROM sales_view s\n",
        "    JOIN products_view p\n",
        "      ON s.product_id = p.product_id\n",
        "    WHERE s.date >= '2024-01-02'\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY5VbOJcVMud",
        "outputId": "34e9d008-6729-4438-c493-63b6854b8c15"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----------+------------+\n",
            "|      date|region|sales|product_id|product_name|\n",
            "+----------+------+-----+----------+------------+\n",
            "|2024-01-02| North|  150|       101|       Phone|\n",
            "|2024-01-02| South|  300|       103|      Laptop|\n",
            "|2024-01-03|  East|  250|       104|     Monitor|\n",
            "|2024-01-03|  West|  180|       105|    Keyboard|\n",
            "|2024-01-04| North|   90|       106|       Mouse|\n",
            "|2024-01-04|  East|  200|       104|     Monitor|\n",
            "|2024-01-05| South|  210|       102|      Tablet|\n",
            "|2024-01-05|  West|  300|       107|     Printer|\n",
            "+----------+------+-----+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Cache and Repartition"
      ],
      "metadata": {
        "id": "vHSm16w_Vbn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache()\n",
        "df.count()\n",
        "\n",
        "df = df.repartition(4, \"region\")"
      ],
      "metadata": {
        "id": "D1fPdRPhVdbf"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Redistributes the DataFrame into 4 partitions.\n",
        ">\n",
        "> Uses region as the partitioning key, so all rows with the same region ideally end up in the same partition.\n",
        ">\n",
        "> Causes a shuffle (data movement across the cluster) â€” can be expensive.\n",
        "\n"
      ],
      "metadata": {
        "id": "EDaZVlZLVuyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Partitions:\", df.rdd.getNumPartitions())\n",
        "df.rdd.glom().map(lambda x: [row.asDict() for row in x]).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGdkPF8iVyRU",
        "outputId": "7372e48f-b0a5-42f6-e12b-5f96bf50c39c"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions: 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'date': datetime.date(2024, 1, 1),\n",
              "   'region': 'North',\n",
              "   'sales': 100,\n",
              "   'product_id': 101},\n",
              "  {'date': datetime.date(2024, 1, 2),\n",
              "   'region': 'North',\n",
              "   'sales': 150,\n",
              "   'product_id': 101},\n",
              "  {'date': datetime.date(2024, 1, 3),\n",
              "   'region': 'West',\n",
              "   'sales': 180,\n",
              "   'product_id': 105},\n",
              "  {'date': datetime.date(2024, 1, 4),\n",
              "   'region': 'North',\n",
              "   'sales': 90,\n",
              "   'product_id': 106},\n",
              "  {'date': datetime.date(2024, 1, 5),\n",
              "   'region': 'West',\n",
              "   'sales': 300,\n",
              "   'product_id': 107}],\n",
              " [{'date': datetime.date(2024, 1, 3),\n",
              "   'region': 'East',\n",
              "   'sales': 250,\n",
              "   'product_id': 104},\n",
              "  {'date': datetime.date(2024, 1, 4),\n",
              "   'region': 'East',\n",
              "   'sales': 200,\n",
              "   'product_id': 104}],\n",
              " [{'date': datetime.date(2024, 1, 1),\n",
              "   'region': 'South',\n",
              "   'sales': 200,\n",
              "   'product_id': 102},\n",
              "  {'date': datetime.date(2024, 1, 2),\n",
              "   'region': 'South',\n",
              "   'sales': 300,\n",
              "   'product_id': 103},\n",
              "  {'date': datetime.date(2024, 1, 5),\n",
              "   'region': 'South',\n",
              "   'sales': 210,\n",
              "   'product_id': 102}],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This shows a list of lists: one inner list per partition, each containing its rows as dictionaries."
      ],
      "metadata": {
        "id": "zCCR4gxOV3c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save to Disk (Parquet)"
      ],
      "metadata": {
        "id": "aShDnJaOV_a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.show()\n",
        "joined_df.write.mode(\"overwrite\").parquet(\"output/joined_sales_products.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JIn6rqTV_zg",
        "outputId": "70aeb5d8-b2d0-4c1f-d0d1-3eb37a646d75"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "|product_id|      date|region|sales|product_name|   category| price|\n",
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "|       101|2024-01-01| North|  100|       Phone|Electronics|599.99|\n",
            "|       102|2024-01-01| South|  200|      Tablet|Electronics|399.99|\n",
            "|       101|2024-01-02| North|  150|       Phone|Electronics|599.99|\n",
            "|       103|2024-01-02| South|  300|      Laptop|Electronics|999.99|\n",
            "|       104|2024-01-03|  East|  250|     Monitor|Accessories|199.99|\n",
            "|       105|2024-01-03|  West|  180|    Keyboard|Accessories| 49.99|\n",
            "|       106|2024-01-04| North|   90|       Mouse|Accessories| 29.99|\n",
            "|       104|2024-01-04|  East|  200|     Monitor|Accessories|199.99|\n",
            "|       102|2024-01-05| South|  210|      Tablet|Electronics|399.99|\n",
            "|       107|2024-01-05|  West|  300|     Printer|     Office|149.99|\n",
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register in Spark catalog"
      ],
      "metadata": {
        "id": "r1Rce2sTn-0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the Parquet file\n",
        "joined_df.write.mode(\"overwrite\").parquet(\"output/sales_products/\")\n",
        "\n",
        "# Register the location as a SQL view\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TEMPORARY VIEW sales_products_view\n",
        "    USING PARQUET\n",
        "    OPTIONS (\n",
        "        path \"output/sales_products/\"\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# Query it\n",
        "spark.sql(\"SELECT * FROM sales_products_view LIMIT 10\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHZDc18ToAwC",
        "outputId": "03e49079-7663-47ee-fef2-09931b36001b"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "|product_id|      date|region|sales|product_name|   category| price|\n",
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "|       101|2024-01-01| North|  100|       Phone|Electronics|599.99|\n",
            "|       102|2024-01-01| South|  200|      Tablet|Electronics|399.99|\n",
            "|       101|2024-01-02| North|  150|       Phone|Electronics|599.99|\n",
            "|       103|2024-01-02| South|  300|      Laptop|Electronics|999.99|\n",
            "|       104|2024-01-03|  East|  250|     Monitor|Accessories|199.99|\n",
            "|       105|2024-01-03|  West|  180|    Keyboard|Accessories| 49.99|\n",
            "|       106|2024-01-04| North|   90|       Mouse|Accessories| 29.99|\n",
            "|       104|2024-01-04|  East|  200|     Monitor|Accessories|199.99|\n",
            "|       102|2024-01-05| South|  210|      Tablet|Electronics|399.99|\n",
            "|       107|2024-01-05|  West|  300|     Printer|     Office|149.99|\n",
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 2: Using Hive Metastore (if enabled)"
      ],
      "metadata": {
        "id": "OBeq5glMogIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataLakeWithCatalog\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "joined_df.write.mode(\"overwrite\").saveAsTable(\"sales_products\")"
      ],
      "metadata": {
        "id": "02K10w48oZc-"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the Hive Table in Spark"
      ],
      "metadata": {
        "id": "MGWvZV-Domir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List available Hive tables\n",
        "spark.sql(\"SHOW TABLES\").show()\n",
        "\n",
        "# See the schema of the table\n",
        "spark.sql(\"DESCRIBE sales_products\").show()\n",
        "\n",
        "# Run a simple query\n",
        "spark.sql(\"SELECT region, SUM(sales) as total_sales FROM sales_products GROUP BY region\").show()\n",
        "\n",
        "# Query with filters\n",
        "spark.sql(\"SELECT * FROM sales_products WHERE category = 'Electronics' AND sales > 500\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nK0QRxOoivV",
        "outputId": "07f5c6c2-4a9e-44af-fdc3-644c8f7f798d"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+-----------+\n",
            "|namespace|          tableName|isTemporary|\n",
            "+---------+-------------------+-----------+\n",
            "|  default|     sales_products|      false|\n",
            "|         |      products_view|       true|\n",
            "|         |sales_products_view|       true|\n",
            "|         |         sales_view|       true|\n",
            "+---------+-------------------+-----------+\n",
            "\n",
            "+------------+---------+-------+\n",
            "|    col_name|data_type|comment|\n",
            "+------------+---------+-------+\n",
            "|  product_id|      int|   NULL|\n",
            "|        date|     date|   NULL|\n",
            "|      region|   string|   NULL|\n",
            "|       sales|      int|   NULL|\n",
            "|product_name|   string|   NULL|\n",
            "|    category|   string|   NULL|\n",
            "|       price|   double|   NULL|\n",
            "+------------+---------+-------+\n",
            "\n",
            "+------+-----------+\n",
            "|region|total_sales|\n",
            "+------+-----------+\n",
            "| South|        710|\n",
            "|  East|        450|\n",
            "|  West|        480|\n",
            "| North|        340|\n",
            "+------+-----------+\n",
            "\n",
            "+----------+----+------+-----+------------+--------+-----+\n",
            "|product_id|date|region|sales|product_name|category|price|\n",
            "+----------+----+------+-----+------------+--------+-----+\n",
            "+----------+----+------+-----+------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save to Disk (CSV)"
      ],
      "metadata": {
        "id": "AvfCgBGwWftM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.write \\\n",
        "    .option(\"header\", True) \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .csv(\"output/joined_sales_products.csv\")"
      ],
      "metadata": {
        "id": "4MwNe9zPWeYP"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 ML"
      ],
      "metadata": {
        "id": "dFGo1B5ubYZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Spark Session"
      ],
      "metadata": {
        "id": "VG_2SX6vbcwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = spark.read.parquet(\"output/joined_sales_products.parquet\")\n",
        "joined_df.show()"
      ],
      "metadata": {
        "id": "WCHxpxmHbaC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29232196-d6f9-4023-eb04-b9e7ea8baeb3"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "|product_id|      date|region|sales|product_name|   category| price|\n",
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "|       101|2024-01-01| North|  100|       Phone|Electronics|599.99|\n",
            "|       102|2024-01-01| South|  200|      Tablet|Electronics|399.99|\n",
            "|       101|2024-01-02| North|  150|       Phone|Electronics|599.99|\n",
            "|       103|2024-01-02| South|  300|      Laptop|Electronics|999.99|\n",
            "|       104|2024-01-03|  East|  250|     Monitor|Accessories|199.99|\n",
            "|       105|2024-01-03|  West|  180|    Keyboard|Accessories| 49.99|\n",
            "|       106|2024-01-04| North|   90|       Mouse|Accessories| 29.99|\n",
            "|       104|2024-01-04|  East|  200|     Monitor|Accessories|199.99|\n",
            "|       102|2024-01-05| South|  210|      Tablet|Electronics|399.99|\n",
            "|       107|2024-01-05|  West|  300|     Printer|     Office|149.99|\n",
            "+----------+----------+------+-----+------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect Schema"
      ],
      "metadata": {
        "id": "dFv-HpF3ey8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9km8nRDezDl",
        "outputId": "93a6c18a-6539-46b3-f44f-9e714f0a6eb4"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- sales: double (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- discount: double (nullable = true)\n",
            " |-- channel: string (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- base_price: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose numeric features"
      ],
      "metadata": {
        "id": "qbZtZcJ9e2Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [\"base_price\", \"quantity\", \"discount\"]"
      ],
      "metadata": {
        "id": "Tk0KVBf9e2of"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then use VectorAssembler"
      ],
      "metadata": {
        "id": "Sb-DYGxLe8GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "assembled_df = assembler.transform(joined_df).select(\"features\", \"sales\")"
      ],
      "metadata": {
        "id": "pzH1o4E2e8OF"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Linear Regression Model"
      ],
      "metadata": {
        "id": "oJSbVqq9fAlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"sales\")\n",
        "lr_model = lr.fit(assembled_df)"
      ],
      "metadata": {
        "id": "SSKL0PVZe_aF"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Predictions"
      ],
      "metadata": {
        "id": "OAu3Fma6fEYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lr_model.transform(assembled_df)\n",
        "predictions.select(\"features\", \"sales\", \"prediction\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEcj8wC9fErn",
        "outputId": "31e3a390-1a24-40c7-fa19-48fc3c477f46"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------+------------------+\n",
            "|          features|  sales|        prediction|\n",
            "+------------------+-------+------------------+\n",
            "|  [98.96,9.0,0.11]| 793.41|1264.6579182142525|\n",
            "| [35.11,10.0,0.18]| 288.47|1122.8630183946618|\n",
            "| [294.36,3.0,0.08]| 816.65| 873.9604500072271|\n",
            "| [444.19,3.0,0.17]|1101.73|1479.7513424578617|\n",
            "| [458.05,9.0,0.06]|3874.64|2971.5559766738897|\n",
            "| [224.47,6.0,0.23]|1038.01|1068.2052035399354|\n",
            "|[155.66,10.0,0.01]|1538.76| 1842.947607524383|\n",
            "| [91.25,10.0,0.13]| 791.03|1430.2080404930382|\n",
            "|  [98.96,7.0,0.03]| 673.72|  900.714979370545|\n",
            "| [470.44,4.0,0.12]| 1651.5|1869.3773679931694|\n",
            "+------------------+-------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "lZN2I0bxfIGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq1V3SByfIOa",
        "outputId": "a192d965-7cc5-42a9-a820-48a060069cc8"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 369.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a DataFrame with the input\n",
        "2. Assemble Features\n",
        "3. Predict with the model"
      ],
      "metadata": {
        "id": "ctAiwDGZfTEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Create a single-row DataFrame with your input\n",
        "input_data = spark.createDataFrame([\n",
        "    Row(base_price=94.0, quantity=9.2, discount=0.11)\n",
        "])\n",
        "input_features = assembler.transform(input_data)  # re-use your VectorAssembler\n",
        "prediction_result = lr_model.transform(input_features)\n",
        "prediction_result.select(\"features\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYxo9GJafTRT",
        "outputId": "f40823d2-685e-4073-beb2-fc25a5694980"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+\n",
            "|       features|       prediction|\n",
            "+---------------+-----------------+\n",
            "|[94.0,9.2,0.11]|1285.817023641747|\n",
            "+---------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-step ML in Spark on sales_products"
      ],
      "metadata": {
        "id": "ZH_YS0CptD-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Step 1: Start Spark session with Hive support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SalesPrediction\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Load the Hive table\n",
        "df = spark.sql(\"SELECT * FROM sales_products\")\n",
        "\n",
        "# Step 3: Handle categorical features\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"region\", outputCol=\"region_index\"),\n",
        "    StringIndexer(inputCol=\"product_name\", outputCol=\"product_index\"),\n",
        "    StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
        "]\n",
        "\n",
        "for indexer in indexers:\n",
        "    df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Step 4: Assemble features\n",
        "feature_cols = [\"region_index\", \"product_index\", \"category_index\", \"price\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "assembled_df = assembler.transform(df).select(\"features\", \"sales\")\n",
        "\n",
        "# Step 5: Train a Linear Regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"sales\")\n",
        "lr_model = lr.fit(assembled_df)\n",
        "\n",
        "# Step 6: Make predictions\n",
        "predictions = lr_model.transform(assembled_df)\n",
        "predictions.select(\"features\", \"sales\", \"prediction\").show(10)\n",
        "\n",
        "# Optional: Evaluate model\n",
        "print(\"Coefficients:\", lr_model.coefficients)\n",
        "print(\"Intercept:\", lr_model.intercept)\n",
        "print(\"RMSE:\", lr_model.summary.rootMeanSquaredError)\n",
        "print(\"R2:\", lr_model.summary.r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8jTBRyjtEFk",
        "outputId": "b9565d39-1797-47ab-cad6-d5d3e46b29fc"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+------------------+\n",
            "|            features|sales|        prediction|\n",
            "+--------------------+-----+------------------+\n",
            "|[0.0,1.0,0.0,599.99]|  100|157.62096040728568|\n",
            "|[1.0,2.0,0.0,399.99]|  200| 171.6961154173007|\n",
            "|[0.0,1.0,0.0,599.99]|  150|157.62096040728568|\n",
            "|[1.0,4.0,0.0,999.99]|  300| 293.0639982675108|\n",
            "|[2.0,0.0,1.0,199.99]|  250| 207.5259402438897|\n",
            "| [3.0,3.0,1.0,49.99]|  180|236.11518672409272|\n",
            "| [0.0,5.0,1.0,29.99]|   90| 85.43663295476075|\n",
            "|[2.0,0.0,1.0,199.99]|  200| 207.5259402438897|\n",
            "|[1.0,2.0,0.0,399.99]|  210| 171.6961154173007|\n",
            "|[3.0,6.0,2.0,149.99]|  300|291.69814991668346|\n",
            "+--------------------+-----+------------------+\n",
            "\n",
            "Coefficients: [50.53102393038454,2.4000552178203036,28.95483547003486,0.19427962069094912]\n",
            "Intercept: 38.65507557110281\n",
            "RMSE: 32.85423022353661\n",
            "R2: 0.7730444820056462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Spark and NLP"
      ],
      "metadata": {
        "id": "F-FwMv_8XF6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load File into DataFrame"
      ],
      "metadata": {
        "id": "MzHST0bHXW2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.text(\"feedback.txt\").withColumnRenamed(\"value\", \"text\")\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yCi_JQDXYk9",
        "outputId": "22bbb01e-fc12-4601-c137-e0148b92e2de"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------+\n",
            "|text                                                    |\n",
            "+--------------------------------------------------------+\n",
            "|Great experience with fast shipping and quality product.|\n",
            "|Unhelpful and rude customer service representative.     |\n",
            "|Helpful agent resolved my issue quickly.                |\n",
            "|The service was excellent and very helpful.             |\n",
            "|Slow response from the support team.                    |\n",
            "|Battery life is poor and needs improvement.             |\n",
            "|Delivery was late and the phone was damaged.            |\n",
            "|I am disappointed with the damaged packaging.           |\n",
            "|Customer support was not helpful at all.                |\n",
            "|Monitor quality is excellent and arrived on time.       |\n",
            "|I am disappointed with the damaged packaging.           |\n",
            "|Battery life is poor and needs improvement.             |\n",
            "|Repeated delivery issues and poor communication.        |\n",
            "|The service was excellent and very helpful.             |\n",
            "|Excellent customer service resolved everything quickly. |\n",
            "|Received the wrong item and had to return it.           |\n",
            "|Customer support was not helpful at all.                |\n",
            "|Service was very slow and unprofessional.               |\n",
            "|Battery life is poor and needs improvement.             |\n",
            "|Unhelpful and rude customer service representative.     |\n",
            "+--------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize Text into Words"
      ],
      "metadata": {
        "id": "JzUZNMueYoN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "tokenized_df = tokenizer.transform(df)\n",
        "tokenized_df.select(\"words\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7l9TvbdYpY5",
        "outputId": "e26bdcc5-739b-4248-c6d9-44365b8397f4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------+\n",
            "|words                                                            |\n",
            "+-----------------------------------------------------------------+\n",
            "|[great, experience, with, fast, shipping, and, quality, product.]|\n",
            "|[unhelpful, and, rude, customer, service, representative.]       |\n",
            "|[helpful, agent, resolved, my, issue, quickly.]                  |\n",
            "|[the, service, was, excellent, and, very, helpful.]              |\n",
            "|[slow, response, from, the, support, team.]                      |\n",
            "|[battery, life, is, poor, and, needs, improvement.]              |\n",
            "|[delivery, was, late, and, the, phone, was, damaged.]            |\n",
            "|[i, am, disappointed, with, the, damaged, packaging.]            |\n",
            "|[customer, support, was, not, helpful, at, all.]                 |\n",
            "|[monitor, quality, is, excellent, and, arrived, on, time.]       |\n",
            "|[i, am, disappointed, with, the, damaged, packaging.]            |\n",
            "|[battery, life, is, poor, and, needs, improvement.]              |\n",
            "|[repeated, delivery, issues, and, poor, communication.]          |\n",
            "|[the, service, was, excellent, and, very, helpful.]              |\n",
            "|[excellent, customer, service, resolved, everything, quickly.]   |\n",
            "|[received, the, wrong, item, and, had, to, return, it.]          |\n",
            "|[customer, support, was, not, helpful, at, all.]                 |\n",
            "|[service, was, very, slow, and, unprofessional.]                 |\n",
            "|[battery, life, is, poor, and, needs, improvement.]              |\n",
            "|[unhelpful, and, rude, customer, service, representative.]       |\n",
            "+-----------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stop Words"
      ],
      "metadata": {
        "id": "8ad7PLkOYryw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_df = remover.transform(tokenized_df)\n",
        "filtered_df.select(\"filtered_words\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3dxRMNHYtnK",
        "outputId": "2bfc18f7-66bf-43ac-93d7-f3757f9fac3d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------+\n",
            "|filtered_words                                                |\n",
            "+--------------------------------------------------------------+\n",
            "|[great, experience, fast, shipping, quality, product.]        |\n",
            "|[unhelpful, rude, customer, service, representative.]         |\n",
            "|[helpful, agent, resolved, issue, quickly.]                   |\n",
            "|[service, excellent, helpful.]                                |\n",
            "|[slow, response, support, team.]                              |\n",
            "|[battery, life, poor, needs, improvement.]                    |\n",
            "|[delivery, late, phone, damaged.]                             |\n",
            "|[disappointed, damaged, packaging.]                           |\n",
            "|[customer, support, helpful, all.]                            |\n",
            "|[monitor, quality, excellent, arrived, time.]                 |\n",
            "|[disappointed, damaged, packaging.]                           |\n",
            "|[battery, life, poor, needs, improvement.]                    |\n",
            "|[repeated, delivery, issues, poor, communication.]            |\n",
            "|[service, excellent, helpful.]                                |\n",
            "|[excellent, customer, service, resolved, everything, quickly.]|\n",
            "|[received, wrong, item, return, it.]                          |\n",
            "|[customer, support, helpful, all.]                            |\n",
            "|[service, slow, unprofessional.]                              |\n",
            "|[battery, life, poor, needs, improvement.]                    |\n",
            "|[unhelpful, rude, customer, service, representative.]         |\n",
            "+--------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explode into One Word per Row (Optional)"
      ],
      "metadata": {
        "id": "7wF6ULcdYv9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "words_df = filtered_df.select(explode(col(\"filtered_words\")).alias(\"word\"))\n",
        "words_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uulIdNzYwRK",
        "outputId": "ff0c6559-ed2c-4573-f769-1b5dc79a3df1"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|           word|\n",
            "+---------------+\n",
            "|          great|\n",
            "|     experience|\n",
            "|           fast|\n",
            "|       shipping|\n",
            "|        quality|\n",
            "|       product.|\n",
            "|      unhelpful|\n",
            "|           rude|\n",
            "|       customer|\n",
            "|        service|\n",
            "|representative.|\n",
            "|        helpful|\n",
            "|          agent|\n",
            "|       resolved|\n",
            "|          issue|\n",
            "|       quickly.|\n",
            "|        service|\n",
            "|      excellent|\n",
            "|       helpful.|\n",
            "|           slow|\n",
            "+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Frequency Count"
      ],
      "metadata": {
        "id": "3snEHVKEY0av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = words_df.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
        "word_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6OES9GLY0uS",
        "outputId": "597559fc-d284-4c16-cf83-229773fd1987"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|       word|count|\n",
            "+-----------+-----+\n",
            "|   delivery|   85|\n",
            "|    service|   64|\n",
            "|       late|   51|\n",
            "|    battery|   45|\n",
            "|   customer|   40|\n",
            "|      phone|   40|\n",
            "|  excellent|   39|\n",
            "|       slow|   34|\n",
            "|    support|   34|\n",
            "|    quality|   34|\n",
            "|    helpful|   29|\n",
            "|      great|   27|\n",
            "|       fast|   27|\n",
            "|       poor|   27|\n",
            "|    arrived|   26|\n",
            "|       life|   23|\n",
            "|       long|   22|\n",
            "|    smooth.|   22|\n",
            "|      lasts|   22|\n",
            "|performance|   22|\n",
            "+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Punctuation and Normalize Text"
      ],
      "metadata": {
        "id": "dfdkHS3KZfaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, lower\n",
        "\n",
        "cleaned_df = df.withColumn(\"text\", lower(regexp_replace(\"text\", \"[^a-zA-Z\\\\s]\", \"\")))\n",
        "cleaned_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ninD3kTZiDx",
        "outputId": "1d82eb77-22fe-4565-ab22-9de072492238"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|great experience ...|\n",
            "|unhelpful and rud...|\n",
            "|helpful agent res...|\n",
            "|the service was e...|\n",
            "|slow response fro...|\n",
            "|battery life is p...|\n",
            "|delivery was late...|\n",
            "|i am disappointed...|\n",
            "|customer support ...|\n",
            "|monitor quality i...|\n",
            "|i am disappointed...|\n",
            "|battery life is p...|\n",
            "|repeated delivery...|\n",
            "|the service was e...|\n",
            "|excellent custome...|\n",
            "|received the wron...|\n",
            "|customer support ...|\n",
            "|service was very ...|\n",
            "|battery life is p...|\n",
            "|unhelpful and rud...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BVLFiBU3ZzeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, NGram\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "tokenized_df = tokenizer.transform(cleaned_df)\n",
        "\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
        "bigrams_df = ngram.transform(tokenized_df)\n",
        "bigrams_df.select(\"bigrams\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGVEOQj9Zqk6",
        "outputId": "c096f09d-7e8e-4bbc-b948-e49e28a39252"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------+\n",
            "|bigrams                                                                                                  |\n",
            "+---------------------------------------------------------------------------------------------------------+\n",
            "|[great experience, experience with, with fast, fast shipping, shipping and, and quality, quality product]|\n",
            "|[unhelpful and, and rude, rude customer, customer service, service representative]                       |\n",
            "|[helpful agent, agent resolved, resolved my, my issue, issue quickly]                                    |\n",
            "|[the service, service was, was excellent, excellent and, and very, very helpful]                         |\n",
            "|[slow response, response from, from the, the support, support team]                                      |\n",
            "|[battery life, life is, is poor, poor and, and needs, needs improvement]                                 |\n",
            "|[delivery was, was late, late and, and the, the phone, phone was, was damaged]                           |\n",
            "|[i am, am disappointed, disappointed with, with the, the damaged, damaged packaging]                     |\n",
            "|[customer support, support was, was not, not helpful, helpful at, at all]                                |\n",
            "|[monitor quality, quality is, is excellent, excellent and, and arrived, arrived on, on time]             |\n",
            "|[i am, am disappointed, disappointed with, with the, the damaged, damaged packaging]                     |\n",
            "|[battery life, life is, is poor, poor and, and needs, needs improvement]                                 |\n",
            "|[repeated delivery, delivery issues, issues and, and poor, poor communication]                           |\n",
            "|[the service, service was, was excellent, excellent and, and very, very helpful]                         |\n",
            "|[excellent customer, customer service, service resolved, resolved everything, everything quickly]        |\n",
            "|[received the, the wrong, wrong item, item and, and had, had to, to return, return it]                   |\n",
            "|[customer support, support was, was not, not helpful, helpful at, at all]                                |\n",
            "|[service was, was very, very slow, slow and, and unprofessional]                                         |\n",
            "|[battery life, life is, is poor, poor and, and needs, needs improvement]                                 |\n",
            "|[unhelpful and, and rude, rude customer, customer service, service representative]                       |\n",
            "+---------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label the data (simple rule-based heuristic)"
      ],
      "metadata": {
        "id": "hoYp1_4XaraL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Add binary sentiment label: 1 = Positive, 0 = Negative\n",
        "labeled_df = df.withColumn(\"label\", when(\n",
        "    col(\"text\").rlike(\"excellent|helpful|happy|great|fast|smooth|impressive|resolved\"),\n",
        "    1\n",
        ").otherwise(0))"
      ],
      "metadata": {
        "id": "Ms3iAgWtart4"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize + Remove Stop Words"
      ],
      "metadata": {
        "id": "pWEpgkWvgJN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "tokenized = tokenizer.transform(labeled_df)\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered = remover.transform(tokenized)\n"
      ],
      "metadata": {
        "id": "YNp3rnxIgJhX"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to TF-IDF features"
      ],
      "metadata": {
        "id": "4Q9FQSr7gNb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "tf_df = tf.transform(filtered)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idf_model = idf.fit(tf_df)\n",
        "tfidf_df = idf_model.transform(tf_df)\n"
      ],
      "metadata": {
        "id": "qMGLxVcOgNil"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Logistic Regression classifier"
      ],
      "metadata": {
        "id": "v-oXu9fEgRNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = lr.fit(tfidf_df)\n"
      ],
      "metadata": {
        "id": "wvpm2fnegRUj"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict sentiment on the same dataset"
      ],
      "metadata": {
        "id": "WZr_KEb2gUS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(tfidf_df)\n",
        "predictions.select(\"text\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iene6G-hgUgt",
        "outputId": "60dcc8c0-9265-4216-b60e-afd07e74e9e0"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------+-----+----------+------------------------------------------+\n",
            "|text                                                    |label|prediction|probability                               |\n",
            "+--------------------------------------------------------+-----+----------+------------------------------------------+\n",
            "|Great experience with fast shipping and quality product.|1    |1.0       |[2.184005874752603E-9,0.9999999978159941] |\n",
            "|Unhelpful and rude customer service representative.     |1    |1.0       |[3.2615955287345573E-9,0.9999999967384045]|\n",
            "|Helpful agent resolved my issue quickly.                |1    |1.0       |[3.0249932894456743E-9,0.9999999969750067]|\n",
            "|The service was excellent and very helpful.             |1    |1.0       |[7.139534972400031E-9,0.999999992860465]  |\n",
            "|Slow response from the support team.                    |0    |0.0       |[0.9999999961748471,3.8251528611255026E-9]|\n",
            "|Battery life is poor and needs improvement.             |0    |0.0       |[0.9999999951600744,4.839925571431536E-9] |\n",
            "|Delivery was late and the phone was damaged.            |0    |0.0       |[0.9999999951710923,4.828907718135156E-9] |\n",
            "|I am disappointed with the damaged packaging.           |0    |0.0       |[0.999999996035632,3.9643679450307445E-9] |\n",
            "|Customer support was not helpful at all.                |1    |1.0       |[5.2828366188872996E-9,0.9999999947171634]|\n",
            "|Monitor quality is excellent and arrived on time.       |1    |1.0       |[3.6319639913329037E-9,0.999999996368036] |\n",
            "+--------------------------------------------------------+-----+----------+------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a text. Create a DataFrame with the input."
      ],
      "metadata": {
        "id": "2YCXrclJglFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Your new sentence\n",
        "input_text = \"The delivery was delayed and took longer than expected.\"\n",
        "\n",
        "# Create DataFrame\n",
        "new_df = spark.createDataFrame([Row(text=input_text)])\n"
      ],
      "metadata": {
        "id": "dIoQTN0zgmXm"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess."
      ],
      "metadata": {
        "id": "PU_QAogCgwto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Load and label text\n",
        "df = spark.read.text(\"feedback_300.txt\").withColumnRenamed(\"value\", \"text\")\n",
        "df = df.withColumn(\"label\", when(col(\"text\").rlike(\"excellent|helpful|happy|great|fast|smooth|impressive|resolved\"), 1).otherwise(0))\n",
        "\n",
        "# NLP pipeline\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Transform training data\n",
        "words_data = tokenizer.transform(df)\n",
        "filtered_data = remover.transform(words_data)\n",
        "tf_data = hashingTF.transform(filtered_data)\n",
        "idf_model = idf.fit(tf_data)\n",
        "train_data = idf_model.transform(tf_data).select(\"features\", \"label\")\n",
        "\n",
        "# Train model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Predict single sentence\n",
        "test_df = spark.createDataFrame([Row(text=\"The delivery was delayed and took longer than expected.\")])\n",
        "test_words = tokenizer.transform(test_df)\n",
        "test_filtered = remover.transform(test_words)\n",
        "test_tf = hashingTF.transform(test_filtered)\n",
        "test_final = idf_model.transform(test_tf)\n",
        "model.transform(test_final.select(\"features\")).select(\"prediction\", \"probability\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6h9Xft-gxj8",
        "outputId": "5bfb37ad-8cc2-4e96-99b2-8f00db1e5bc4"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------------------------------+\n",
            "|prediction|probability                             |\n",
            "+----------+----------------------------------------+\n",
            "|0.0       |[0.9862279669633595,0.01377203303664054]|\n",
            "+----------+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark commands"
      ],
      "metadata": {
        "id": "x_KmIsfekTnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Frequency Analysis with PySpark DataFrames\n",
        "\n",
        "This task processes a plain text file (feedback.txt) using PySparkâ€™s DataFrame API to perform a distributed word count. It reads each line of text, splits the content into lowercase words, filters out empty or non-word tokens, counts how often each word appears, and displays the results sorted by frequency.\n",
        "\n",
        "The full pipeline includes:\n",
        "\n",
        "1. Loading the text file as a DataFrame.\n",
        "\n",
        "2. Tokenizing and normalizing each line into individual lowercase words.\n",
        "\n",
        "3. Removing noise, such as empty tokens or punctuation-only entries.\n",
        "\n",
        "4. Counting occurrences of each word using groupBy().count().\n",
        "\n",
        "5. Displaying results, ordered by frequency in descending order.\n",
        "\n",
        "This is a typical entry-level NLP/data-cleaning task for log analysis, sentiment preprocessing, or indexing pipelines â€” easily scalable with Spark."
      ],
      "metadata": {
        "id": "jqnUn-b7kVQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word_count_df.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, lower, col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCountDF\").getOrCreate()\n",
        "\n",
        "# Read the file into a DataFrame (each line is one row)\n",
        "df = spark.read.text(\"feedback.txt\").withColumnRenamed(\"value\", \"line\")\n",
        "\n",
        "# Split into words, convert to lowercase, and explode into individual rows\n",
        "words_df = df.select(explode(split(lower(col(\"line\")), r\"\\W+\")).alias(\"word\"))\n",
        "\n",
        "# Remove empty words (from punctuation, extra spaces, etc.)\n",
        "words_df = words_df.filter(col(\"word\") != \"\")\n",
        "\n",
        "# Group and count word occurrences\n",
        "word_counts_df = words_df.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
        "\n",
        "# Show top results\n",
        "word_counts_df.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "K_bBcPVakwHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```spark-submit your_script.py\n",
        "```"
      ],
      "metadata": {
        "id": "M_W79OSdkY4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "spark-submit --master local[4] \\\n",
        "             --executor-memory 2g \\\n",
        "             --conf spark.sql.shuffle.partitions=10 \\\n",
        "             word_count.py feedback_300.txt\n",
        "            ```"
      ],
      "metadata": {
        "id": "l6x5J3YHkak5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`spark-submit --version`"
      ],
      "metadata": {
        "id": "OEZQjA4MkfEc"
      }
    }
  ]
}